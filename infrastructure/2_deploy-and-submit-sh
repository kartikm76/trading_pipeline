#!/bin/bash
set -e

# --- 1. CAPTURE ARGUMENT ---
# Usage: ./deploy-and-submit.sh [bootstrap|daily]
MODE=${1:-daily}
IS_BOOTSTRAP="false"
MAX_EXECUTORS=20
EXEC_MEM="8G"

if [ "$MODE" == "bootstrap" ]; then
    echo "ðŸš¨ BOOTSTRAP MODE DETECTED: Scaling up for 200GB load..."
    IS_BOOTSTRAP="true"
    MAX_EXECUTORS=100
    EXEC_MEM="16G"
fi

# --- 2. CONFIGURATION ---
S3_BUCKET="trading-pipeline"
AWS_REGION="us-east-1"
DIST_DIR="./dist"
LAST_HASH_FILE=".last_pyproject_hash"
mkdir -p $DIST_DIR

# --- 3. PACKAGING (Source & Dependencies) ---
# [Logic from your deploy-quick.sh remains here for UV/Docker packaging]
PYPROJECT_HASH=$(shasum pyproject.toml | awk '{ print $1 }')
if [ ! -f "$DIST_DIR/pyspark_deps.tar.gz" ] || [ "$PYPROJECT_HASH" != "$(cat $LAST_HASH_FILE)" ]; then
    echo "ðŸ“¦ Rebuilding dependencies..."
    # (Existing Docker/UV command here)
    echo "$PYPROJECT_HASH" > "$LAST_HASH_FILE"
fi

echo "ðŸ“‚ Packaging source code..."
zip -r $DIST_DIR/src.zip src/ -x "*.pyc" "__pycache__/*"
aws s3 cp $DIST_DIR/src.zip s3://$S3_BUCKET/artifacts/
aws s3 cp config.yaml s3://$S3_BUCKET/artifacts/
aws s3 cp src/main.py s3://$S3_BUCKET/artifacts/

# --- 4. EMR SERVERLESS SUBMISSION ---
APP_ID=$(cat .application-id)
EXECUTION_ROLE_ARN="arn:aws:iam::YOUR_ACCOUNT_ID:role/EMRServerlessRole"

echo "ðŸš€ Submitting $MODE job to Application $APP_ID..."

JOB_RUN_ID=$(aws emr-serverless start-job-run \
  --region ${AWS_REGION} \
  --application-id ${APP_ID} \
  --execution-role-arn ${EXECUTION_ROLE_ARN} \
  --job-driver '{
    "sparkSubmit": {
      "entryPoint": "s3://'${S3_BUCKET}'/artifacts/main.py",
      "sparkSubmitParameters": "--conf spark.app.is_bootstrap='${IS_BOOTSTRAP}' \
        --conf spark.dynamicAllocation.maxExecutors='${MAX_EXECUTORS}' \
        --conf spark.emr-serverless.executor.memory='${EXEC_MEM}' \
        --conf spark.archives=s3://'${S3_BUCKET}'/artifacts/pyspark_deps.tar.gz#environment \
        --conf spark.submit.pyFiles=s3://'${S3_BUCKET}'/artifacts/src.zip \
        --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
        --conf spark.sql.catalog.glue_catalog=org.apache.iceberg.spark.SparkCatalog \
        --conf spark.sql.catalog.glue_catalog.catalog-impl=org.apache.iceberg.aws.glue.GlueCatalog \
        --conf spark.sql.catalog.glue_catalog.io-impl=org.apache.iceberg.aws.s3.S3FileIO \
        --conf spark.jars=/usr/share/aws/iceberg/lib/iceberg-spark-runtime-3.3_2.12-1.3.0.jar"
    }
  }' --query 'jobRunId' --output text)

echo "âœ… Job submitted: $JOB_RUN_ID"